{
  "id": 20163914,
  "updated": "2019-02-18T19:03:06Z",
  "additionalIndexing": "34",
  "affairType": {
    "abbreviation": "Po.",
    "id": 6,
    "name": "Postulat"
  },
  "author": {
    "councillor": {
      "code": 3060,
      "gender": "m",
      "id": 4182,
      "name": "Béglé Claude",
      "officialDenomination": "Béglé"
    },
    "faction": {
      "abbreviation": "Groupe C",
      "id": 3,
      "name": "Groupe PDC"
    },
    "type": "author"
  },
  "deposit": {
    "council": {
      "abbreviation": "CN",
      "id": 1,
      "name": "Conseil national",
      "type": "N"
    },
    "date": "2016-11-28T00:00:00Z",
    "legislativePeriod": 50,
    "session": "5006"
  },
  "descriptors": [

  ],
  "drafts": [
    {
      "consultation": {
        "resolutions": [
          {
            "category": {
              "id": 3,
              "name": "Normal"
            },
            "council": {
              "abbreviation": "CN",
              "id": 1,
              "name": "Conseil national",
              "type": "N"
            },
            "date": "2018-02-28T00:00:00Z",
            "text": "Rejet",
            "type": 22
          }
        ]
      },
      "federalCouncilProposal": {
        "code": "-",
        "date": "2017-01-25T00:00:00Z",
        "text": "Le Conseil fédéral propose de rejeter le postulat."
      },
      "index": 0,
      "links": [

      ],
      "preConsultations": [

      ],
      "references": [

      ],
      "relatedDepartments": [
        {
          "abbreviation": "DFF",
          "id": 7,
          "name": "Département des finances",
          "leading": true
        }
      ],
      "states": [
        {
          "date": "/Date(1480287600000+0100)/",
          "id": 24,
          "name": "Non encore traité au conseil"
        },
        {
          "date": "/Date(1519772400000+0100)/",
          "id": 27,
          "name": "Liquidé"
        }
      ],
      "texts": [

      ]
    }
  ],
  "handling": {
    "date": "2018-02-28T00:00:00Z",
    "legislativePeriod": 50,
    "session": "5012"
  },
  "language": "fr",
  "priorityCouncils": [
    {
      "abbreviation": "CN",
      "id": 1,
      "name": "Conseil national",
      "type": "N",
      "priority": 1
    }
  ],
  "relatedAffairs": [

  ],
  "roles": [
    {
      "councillor": {
        "code": 3060,
        "gender": "m",
        "id": 4182,
        "name": "Béglé Claude",
        "officialDenomination": "Béglé"
      },
      "faction": {
        "abbreviation": "Groupe C",
        "id": 3,
        "name": "Groupe PDC"
      },
      "type": "author"
    }
  ],
  "shortId": "16.3914",
  "state": {
    "id": 27,
    "name": "Liquidé",
    "doneKey": "1",
    "newKey": 0
  },
  "texts": [
    {
      "type": {
        "id": 6,
        "name": "Développement"
      },
      "value": "<p>C'est grâce à un algorithme que nos recherches sur Internet aboutissent à un résultat. Les algorithmes sont des séries d'instructions permettant de hiérarchiser les informations que consomment des milliards de personnes. Quelle est l'information qui arrivera en premier sur l'écran? C'est toute la question. Cette hiérarchisation reflète un système de valeur, une vision du monde.</p><p>Aujourd'hui, ce sont des sociétés privées qui font ces choix. L'information peut être hiérarchisée selon quatre critères: la popularité (nombre de visites sur un site), l'autorité (références/liens hypertextes), la réputation (nombre de \"retweet\", de \"like\"), la prédiction du comportement grâce aux traces laissées sur Internet. Or, cette hiérarchisation présente des risques.</p><p>1. Les algorithmes ont tendance à restreindre le principe de liberté.</p><p>Afin de fidéliser l'internaute, ils privilégient l'information la plus populaire et les informations conformes à ses opinions tout en écartant les autres créant ainsi des \"bulle de filtres\". La menace pour la démocratie est d'autant plus sérieuse qu'une part croissante des citoyens délaisse les médias de qualité pour s'informer sur les réseaux sociaux.</p><p>2. Les algorithmes ont tendance à exacerber les inégalités.</p><p>En globalisant les marchés du jugement, les algorithmes donnent aux \"meilleurs\" une visibilité surnuméraire. Le reste, et notamment la \"moyenne\", risque d'être oublié: 95 pour cent de l'audience d'Internet est centrée sur 0,03 pour cent des contenus (selon le sociologue Dominique Cardon).</p><p>3. Cela peut entraîner une discrimination.</p><p>Le \"dynamic pricing\", reposant sur des algorithmes, peut pénaliser les clients fidèles, pressés ou captifs avec des prix plus élevés et exclure les clients \"sans potentiel\" des meilleures offres.</p><p>Une plus grande transparence et une clarification des responsabilités permettraient de limiter le pouvoir politique, économique et social des algorithmes.</p>"
    },
    {
      "type": {
        "id": 14,
        "name": "Réponse CF / Bureau"
      },
      "value": "<p>De nos jours, consulter des plates-formes Internet spécialisées dans les recherches, les rencontres, les médias, les évaluations et les échanges est devenue une activité courante de la vie moderne. Ces plates-formes ont changé notre vie quotidienne, et leur influence sur les valeurs fondamentales de notre société ne cesse de croître. Le fonctionnement de ces plates-formes et systèmes informatiques se fonde sur des algorithmes, qui permettent de procéder au traitement des données requis. Vu cette évolution, le Conseil fédéral partage l'opinion de l'auteur du postulat, selon lequel il convient d'examiner les risques potentiels liés aux algorithmes si l'on entend tirer durablement parti des chances que ceux-ci nous offrent. Toutefois, il importe de ne pas considérer les algorithmes isolément, mais de les examiner dans le contexte général que constituent le traitement des données ainsi que le fonctionnement visé des systèmes et plates-formes informatiques. C'est pourquoi le Conseil fédéral a décidé de prendre diverses mesures.</p><p>- Instituée dans le cadre de la mise en oeuvre de la motion Rechsteiner Paul 13.3841, \"Commission d'experts pour l'avenir du traitement et de la sécurité des données\", la commission d'experts pour l'avenir du traitement et de la sécurité des données étudie la question des algorithmes sous différents points de vue. Elle examine, outre des thèmes spécifiques tels que la manipulation numérique (\"Big Nudging\", bulle de filtres, etc.), des problématiques transversales. Au nombre de ces problématiques figure la question de savoir dans quelle mesure des principes éthiques, combinés à des exigences légales, seraient susceptibles d'empêcher que des abus n'affectent le traitement de données en général et le jeu des algorithmes en particulier, et le cas échéant, quels principes éthiques pourraient entrer en ligne de compte. La commission d'experts présentera son rapport et ses recommandations en la matière au milieu de l'année 2018.</p><p>- En ce qui concerne les données personnelles, la révision en cours de la loi sur la protection des données tient compte des différentes situations dans lesquelles le traitement de données personnelles s'effectue par le biais d'algorithmes. Une obligation d'informer et d'auditionner la personne concernée devra ainsi être observée lorsqu'une décision est prise sur la base exclusive d'un système automatisé de traitement de données et que des conséquences juridiques ou des répercussions importantes sont attendues. Grâce au droit d'être informée qui lui est conféré, la personne concernée devra être en mesure d'exiger de plus amples informations sur la décision prise ainsi que sur les modalités et les conséquences de celle-ci. Le projet de révision prévoit également des mesures relatives au profilage, lequel se fonde souvent sur des algorithmes. Enfin, les responsables seront tenus de procéder à une analyse d'impact relative à la protection des données dans les cas où le traitement des données serait susceptible de porter atteinte à la personnalité ou aux droits fondamentaux de la personne concernée. L'opportunité de mesures visant la protection de la personne concernée sera également examinée dans le cadre de la révision.</p><p>- Recherche et formation: en 2015, le Conseil fédéral a lancé le programme national de recherche \"Big Data\" (PNR 75). A la fin de l'an dernier, plusieurs projets traitant de questions d'ordre éthique touchant le domaine du traitement des données et la thématique du \"Big Data\" ont été approuvés dans le cadre du module 2 (mesures sociales, réglementaires et éducationnelles).</p><p>Les mesures énumérées montrent que la question des algorithmes et de l'éthique sur Internet est déjà prise en considération dans les analyses en cours. Le Conseil fédéral considère que la manière la plus efficace d'aborder la question consiste à poursuivre et à renforcer ces analyses.</p>"
    },
    {
      "type": {
        "id": 5,
        "name": "Texte déposé"
      },
      "value": "<p>Il est demandé au Conseil fédéral d'étudier ce qui ce qui peut être attendu et/ou exigé d'un algorithme suisse et étranger en termes d'éthique. En effet, les algorithmes sont opaques, leur responsabilité est floue, leurs obligations légales sont limitées. Comment fonctionnent-t-ils? A qui s'adresser en cas d'information erronée? Relèvent-ils de la loi suisse? Il s'agit de canaliser le pouvoir grandissant des algorithmes tout en préservant les bénéfices qu'ils apportent.</p>"
    },
    {
      "type": {
        "id": 1,
        "name": "Titre de l'objet"
      },
      "value": "Comment introduire de l'éthique dans les algorithmes?"
    }
  ],
  "title": "Comment introduire de l'éthique dans les algorithmes?"
}